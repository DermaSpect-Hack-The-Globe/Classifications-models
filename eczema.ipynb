{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eczema.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBgj-Qde817a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "\n",
        "import tensorflow as tf\n",
        "from skimage import exposure\n",
        "#from tensorflow.contrib.layers import flatten\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.layers import Flatten\n",
        "\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Lambda, ELU\n",
        "from keras.layers.convolutional import Convolution2D, Cropping2D\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "import cv2\n",
        "import glob"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL9vEr4H86ro"
      },
      "source": [
        "def rotateImage(img, angle):\n",
        "    (rows, cols, ch) = img.shape\n",
        "    M = cv2.getRotationMatrix2D((cols/2,rows/2), angle, 1)\n",
        "    return cv2.warpAffine(img, M, (cols,rows))\n",
        "    \n",
        "    \n",
        "def loadBlurImg(path, imgSize):\n",
        "    img = cv2.imread(path)\n",
        "    angle = np.random.randint(0, 360)\n",
        "    img = rotateImage(img, angle)\n",
        "    img = cv2.blur(img,(5,5))\n",
        "    img = cv2.resize(img, imgSize)\n",
        "    return img\n",
        "\n",
        "def loadImgClass(classPath, classLable, classSize, imgSize):\n",
        "    x = []\n",
        "    y = []\n",
        "    \n",
        "    for path in classPath:\n",
        "        img = loadBlurImg(path, imgSize)        \n",
        "        x.append(img)\n",
        "        y.append(classLable)\n",
        "        \n",
        "    while len(x) < classSize:\n",
        "        randIdx = np.random.randint(0, len(classPath))\n",
        "        img = loadBlurImg(classPath[randIdx], imgSize)\n",
        "        x.append(img)\n",
        "        y.append(classLable)\n",
        "        \n",
        "    return x, y\n",
        "\n",
        "def loadData(img_size, classSize):\n",
        "    hotdogs = glob.glob('/content/drive/MyDrive/Eczema - Color (1)/**/*.jpeg', recursive=True) #CHANGE PATH IMPORTAMT: DONT CHANGE '/**/*.jpeg\n",
        "    notHotdogs = glob.glob('/content/drive/MyDrive/ECZEMA-FAIR/**/*.jpeg', recursive=True)#CHANGE PATH IMPORTAMT: DONT CHANGE '/**/*.jpeg\n",
        "    \n",
        "    \n",
        "    imgSize = (img_size, img_size)\n",
        "    xHotdog, yHotdog = loadImgClass(hotdogs, 0, classSize, imgSize)\n",
        "    xNotHotdog, yNotHotdog = loadImgClass(notHotdogs, 1, classSize, imgSize)\n",
        "    print(\"There are\", len(xHotdog), \"psoriasis images\")\n",
        "    print(\"There are\", len(xNotHotdog), \"not psoriasis images\")\n",
        "    \n",
        "    X = np.array(xHotdog + xNotHotdog)\n",
        "    y = np.array(yHotdog + yNotHotdog)\n",
        "    \n",
        "    return X, y"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VL0qD6H8-AY",
        "outputId": "4f0bfde6-c0ab-4055-f0d8-a59b07e930e5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1UBs7LZ9DEB"
      },
      "source": [
        "def toGray(images):\n",
        "    # rgb2gray converts RGB values to grayscale values by forming a weighted sum of the R, G, and B components:\n",
        "    # 0.2989 * R + 0.5870 * G + 0.1140 * B \n",
        "    # source: https://www.mathworks.com/help/matlab/ref/rgb2gray.html\n",
        "    \n",
        "    images = 0.2989*images[:,:,:,0] + 0.5870*images[:,:,:,1] + 0.1140*images[:,:,:,2]\n",
        "    return images\n",
        "\n",
        "def normalizeImages(images):\n",
        "    # use Histogram equalization to get a better range\n",
        "    # source http://scikit-image.org/docs/dev/api/skimage.exposure.html#skimage.exposure.equalize_hist\n",
        "    images = (images / 255.).astype(np.float32)\n",
        "    \n",
        "    for i in range(images.shape[0]):\n",
        "        images[i] = exposure.equalize_hist(images[i])\n",
        "    \n",
        "    images = images.reshape(images.shape + (1,)) \n",
        "    return images\n",
        "\n",
        "def preprocessData(images):\n",
        "    grayImages = toGray(images)\n",
        "    return normalizeImages(grayImages)\n",
        "\n",
        "def normalizeImages2(images):\n",
        "    for i in range(images.shape[0]):\n",
        "        cv2.normalize(images[i],images[i], alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "    \n",
        "    # if convert to gray scale use this after\n",
        "    print(\"images has shape before\", images.shape)\n",
        "    #images = images.reshape(images.shape + (1,)) \n",
        "    #print(\"images has shape after\", images.shape)\n",
        "    return images"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLJlLMWz9JvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be61122-bd40-4464-fad8-2c9df30194a6"
      },
      "source": [
        "size = 128 #image size\n",
        "classSize = 15000 #number of pictures i want for class YES and class NO .... CHANGE TO 15000 OR 20000\n",
        "\n",
        "scaled_X, y = loadData(size, classSize)\n",
        "scaled_X = preprocessData(scaled_X)\n",
        "\n",
        "n_classes = len(np.unique(y))\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "y = to_categorical(y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 15000 psoriasis images\n",
            "There are 15000 not psoriasis images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UJIfhml9J4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f206692-d04e-4f7e-d4bc-dfa659dd8921"
      },
      "source": [
        "rand_state = np.random.randint(0, 100)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=rand_state)\n",
        "\n",
        "print(\"Number of classes =\", n_classes)\n",
        "print(\"train shape X\", X_train.shape)\n",
        "print(\"train shape y\", y_train.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of classes = 2\n",
            "train shape X (24000, 128, 128, 1)\n",
            "train shape y (24000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQvxDy1-9UB0"
      },
      "source": [
        "def karasModel(inputShape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(16, 8, 8, subsample=(4, 4),border_mode='valid', input_shape=inputShape))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(32, 5, 5))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(32, 3, 3))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(240))\n",
        "\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(120))\n",
        "\n",
        "    #model.add(Activation('relu'))\n",
        "    model.add(Dense(2))\n",
        "\n",
        "    model.add(Activation('softmax'))\n",
        "    return model \n",
        "\n",
        "def karasModel2(inputShape):\n",
        "    model = Sequential()\n",
        "    #model.add(Conv2D(16, 8, 8, subsample=(4, 4),border_mode='valid', input_shape=inputShape))\n",
        "    model.add(Conv2D(16, 8, 8, padding='valid', input_shape=inputShape))\n",
        "    model.add(ELU())\n",
        "    model.add(Conv2D(32, 5, 5, padding=\"same\"))\n",
        "    model.add(ELU())\n",
        "    model.add(Conv2D(64, 5, 5, padding=\"same\"))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(.2))\n",
        "    model.add(ELU())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(ELU())\n",
        "    model.add(Dense(2))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e97v9NnE9YrA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f117872d-a606-4e1f-fe7a-90acf617d8d1"
      },
      "source": [
        "from keras.layers.convolutional import Conv2D\n",
        "\n",
        "inputShape = (size, size, 1)\n",
        "model = karasModel2(inputShape)\n",
        "\n",
        "\n",
        "model.compile('adam', 'categorical_crossentropy', ['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=60, validation_split=0.1) #CHANGE EPOCHS TO 60 OR 70\n",
        "\n",
        "\n",
        "metrics = model.evaluate(X_test, y_test)\n",
        "for metric_i in range(len(model.metrics_names)):\n",
        "    metric_name = model.metrics_names[metric_i]\n",
        "    metric_value = metrics[metric_i]\n",
        "    print('{}: {}'.format(metric_name, metric_value))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.6873 - accuracy: 0.5346 - val_loss: 0.6351 - val_accuracy: 0.6321\n",
            "Epoch 2/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.6346 - accuracy: 0.6368 - val_loss: 0.6218 - val_accuracy: 0.6558\n",
            "Epoch 3/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.6087 - accuracy: 0.6667 - val_loss: 0.6172 - val_accuracy: 0.6642\n",
            "Epoch 4/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.5865 - accuracy: 0.6895 - val_loss: 0.5976 - val_accuracy: 0.6721\n",
            "Epoch 5/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.5665 - accuracy: 0.7028 - val_loss: 0.5890 - val_accuracy: 0.6879\n",
            "Epoch 6/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.5409 - accuracy: 0.7225 - val_loss: 0.5768 - val_accuracy: 0.6996\n",
            "Epoch 7/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.5040 - accuracy: 0.7534 - val_loss: 0.5645 - val_accuracy: 0.7204\n",
            "Epoch 8/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.4704 - accuracy: 0.7751 - val_loss: 0.5595 - val_accuracy: 0.7317\n",
            "Epoch 9/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.4396 - accuracy: 0.7964 - val_loss: 0.6747 - val_accuracy: 0.6837\n",
            "Epoch 10/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.4022 - accuracy: 0.8155 - val_loss: 0.5017 - val_accuracy: 0.7742\n",
            "Epoch 11/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.3623 - accuracy: 0.8390 - val_loss: 0.5367 - val_accuracy: 0.7783\n",
            "Epoch 12/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.3231 - accuracy: 0.8622 - val_loss: 0.5553 - val_accuracy: 0.7842\n",
            "Epoch 13/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.2935 - accuracy: 0.8752 - val_loss: 0.5227 - val_accuracy: 0.8012\n",
            "Epoch 14/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.2591 - accuracy: 0.8925 - val_loss: 0.4827 - val_accuracy: 0.8238\n",
            "Epoch 15/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.2274 - accuracy: 0.9088 - val_loss: 0.5779 - val_accuracy: 0.8196\n",
            "Epoch 16/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.1986 - accuracy: 0.9245 - val_loss: 0.5642 - val_accuracy: 0.8400\n",
            "Epoch 17/60\n",
            "675/675 [==============================] - 12s 17ms/step - loss: 0.1708 - accuracy: 0.9314 - val_loss: 0.5685 - val_accuracy: 0.8375\n",
            "Epoch 18/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.1526 - accuracy: 0.9407 - val_loss: 0.6589 - val_accuracy: 0.8467\n",
            "Epoch 19/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.1389 - accuracy: 0.9455 - val_loss: 0.5541 - val_accuracy: 0.8533\n",
            "Epoch 20/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.1274 - accuracy: 0.9513 - val_loss: 0.5261 - val_accuracy: 0.8750\n",
            "Epoch 21/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.1137 - accuracy: 0.9585 - val_loss: 0.6269 - val_accuracy: 0.8646\n",
            "Epoch 22/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0953 - accuracy: 0.9625 - val_loss: 0.5813 - val_accuracy: 0.8813\n",
            "Epoch 23/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0949 - accuracy: 0.9645 - val_loss: 0.7068 - val_accuracy: 0.8629\n",
            "Epoch 24/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.0855 - accuracy: 0.9679 - val_loss: 0.6039 - val_accuracy: 0.8817\n",
            "Epoch 25/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.0835 - accuracy: 0.9696 - val_loss: 0.7970 - val_accuracy: 0.8642\n",
            "Epoch 26/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0833 - accuracy: 0.9710 - val_loss: 0.6711 - val_accuracy: 0.8754\n",
            "Epoch 27/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.0658 - accuracy: 0.9774 - val_loss: 0.7920 - val_accuracy: 0.8721\n",
            "Epoch 28/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0770 - accuracy: 0.9728 - val_loss: 0.7804 - val_accuracy: 0.8754\n",
            "Epoch 29/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.0673 - accuracy: 0.9780 - val_loss: 0.6285 - val_accuracy: 0.8892\n",
            "Epoch 30/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0616 - accuracy: 0.9763 - val_loss: 0.6929 - val_accuracy: 0.8779\n",
            "Epoch 31/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.0624 - accuracy: 0.9795 - val_loss: 0.7620 - val_accuracy: 0.8879\n",
            "Epoch 32/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0630 - accuracy: 0.9775 - val_loss: 0.6700 - val_accuracy: 0.8875\n",
            "Epoch 33/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0635 - accuracy: 0.9776 - val_loss: 0.6712 - val_accuracy: 0.8838\n",
            "Epoch 34/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0551 - accuracy: 0.9797 - val_loss: 0.8811 - val_accuracy: 0.8692\n",
            "Epoch 35/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0559 - accuracy: 0.9798 - val_loss: 0.7412 - val_accuracy: 0.8871\n",
            "Epoch 36/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.0511 - accuracy: 0.9828 - val_loss: 0.7094 - val_accuracy: 0.8833\n",
            "Epoch 37/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.0461 - accuracy: 0.9831 - val_loss: 0.7041 - val_accuracy: 0.8842\n",
            "Epoch 38/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0502 - accuracy: 0.9810 - val_loss: 0.8572 - val_accuracy: 0.8746\n",
            "Epoch 39/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0556 - accuracy: 0.9801 - val_loss: 0.8077 - val_accuracy: 0.8717\n",
            "Epoch 40/60\n",
            "675/675 [==============================] - 10s 15ms/step - loss: 0.0455 - accuracy: 0.9837 - val_loss: 1.0240 - val_accuracy: 0.8800\n",
            "Epoch 41/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0525 - accuracy: 0.9812 - val_loss: 0.8292 - val_accuracy: 0.8871\n",
            "Epoch 42/60\n",
            "675/675 [==============================] - 10s 15ms/step - loss: 0.0467 - accuracy: 0.9841 - val_loss: 0.8051 - val_accuracy: 0.8767\n",
            "Epoch 43/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0393 - accuracy: 0.9868 - val_loss: 0.9636 - val_accuracy: 0.8758\n",
            "Epoch 44/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0483 - accuracy: 0.9833 - val_loss: 1.0076 - val_accuracy: 0.8675\n",
            "Epoch 45/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0413 - accuracy: 0.9856 - val_loss: 0.8289 - val_accuracy: 0.8921\n",
            "Epoch 46/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0412 - accuracy: 0.9870 - val_loss: 0.8365 - val_accuracy: 0.8833\n",
            "Epoch 47/60\n",
            "675/675 [==============================] - 10s 15ms/step - loss: 0.0436 - accuracy: 0.9862 - val_loss: 0.8385 - val_accuracy: 0.8813\n",
            "Epoch 48/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0383 - accuracy: 0.9863 - val_loss: 0.8050 - val_accuracy: 0.8883\n",
            "Epoch 49/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0349 - accuracy: 0.9891 - val_loss: 0.9054 - val_accuracy: 0.8875\n",
            "Epoch 50/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0433 - accuracy: 0.9857 - val_loss: 0.7658 - val_accuracy: 0.8921\n",
            "Epoch 51/60\n",
            "675/675 [==============================] - 10s 15ms/step - loss: 0.0375 - accuracy: 0.9873 - val_loss: 1.0680 - val_accuracy: 0.8629\n",
            "Epoch 52/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0445 - accuracy: 0.9853 - val_loss: 0.9063 - val_accuracy: 0.8888\n",
            "Epoch 53/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0384 - accuracy: 0.9882 - val_loss: 0.9542 - val_accuracy: 0.8821\n",
            "Epoch 54/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0399 - accuracy: 0.9855 - val_loss: 0.9612 - val_accuracy: 0.8737\n",
            "Epoch 55/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.0348 - accuracy: 0.9886 - val_loss: 0.9522 - val_accuracy: 0.8833\n",
            "Epoch 56/60\n",
            "675/675 [==============================] - 9s 14ms/step - loss: 0.0318 - accuracy: 0.9896 - val_loss: 0.9718 - val_accuracy: 0.8992\n",
            "Epoch 57/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0391 - accuracy: 0.9863 - val_loss: 1.0292 - val_accuracy: 0.8871\n",
            "Epoch 58/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0436 - accuracy: 0.9856 - val_loss: 1.0182 - val_accuracy: 0.8721\n",
            "Epoch 59/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0339 - accuracy: 0.9896 - val_loss: 1.1153 - val_accuracy: 0.8892\n",
            "Epoch 60/60\n",
            "675/675 [==============================] - 10s 14ms/step - loss: 0.0337 - accuracy: 0.9887 - val_loss: 0.9218 - val_accuracy: 0.8813\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 0.8970 - accuracy: 0.8782\n",
            "loss: 0.8969516754150391\n",
            "accuracy: 0.878166675567627\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}