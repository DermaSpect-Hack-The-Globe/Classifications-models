{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eczema.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBgj-Qde817a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "\n",
        "import tensorflow as tf\n",
        "from skimage import exposure\n",
        "#from tensorflow.contrib.layers import flatten\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.layers import Flatten\n",
        "\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Lambda, ELU\n",
        "from keras.layers.convolutional import Convolution2D, Cropping2D\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "import cv2\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL9vEr4H86ro"
      },
      "source": [
        "def rotateImage(img, angle):\n",
        "    (rows, cols, ch) = img.shape\n",
        "    M = cv2.getRotationMatrix2D((cols/2,rows/2), angle, 1)\n",
        "    return cv2.warpAffine(img, M, (cols,rows))\n",
        "    \n",
        "    \n",
        "def loadBlurImg(path, imgSize):\n",
        "    img = cv2.imread(path)\n",
        "    angle = np.random.randint(0, 360)\n",
        "    img = rotateImage(img, angle)\n",
        "    img = cv2.blur(img,(5,5))\n",
        "    img = cv2.resize(img, imgSize)\n",
        "    return img\n",
        "\n",
        "def loadImgClass(classPath, classLable, classSize, imgSize):\n",
        "    x = []\n",
        "    y = []\n",
        "    \n",
        "    for path in classPath:\n",
        "        img = loadBlurImg(path, imgSize)        \n",
        "        x.append(img)\n",
        "        y.append(classLable)\n",
        "        \n",
        "    while len(x) < classSize:\n",
        "        randIdx = np.random.randint(0, len(classPath))\n",
        "        img = loadBlurImg(classPath[randIdx], imgSize)\n",
        "        x.append(img)\n",
        "        y.append(classLable)\n",
        "        \n",
        "    return x, y\n",
        "\n",
        "def loadData(img_size, classSize):\n",
        "    eczema = glob.glob('/content/drive/MyDrive/Eczema - Color (1)/**/*.jpeg', recursive=True) #CHANGE PATH IMPORTAMT: DONT CHANGE '/**/*.jpeg\n",
        "    eczema_fair = glob.glob('/content/drive/MyDrive/ECZEMA-FAIR/**/*.jpeg', recursive=True)#CHANGE PATH IMPORTAMT: DONT CHANGE '/**/*.jpeg\n",
        "    \n",
        "    \n",
        "    imgSize = (img_size, img_size)\n",
        "    xEczema, yEczema = loadImgClass(eczema, 0, classSize, imgSize)\n",
        "    xEczemaFair, yEczemaFair = loadImgClass(eczema_fair, 1, classSize, imgSize)\n",
        "    print(\"There are\", len(xEczema), \"psoriasis images\")\n",
        "    print(\"There are\", len(x), \"not psoriasis images\")\n",
        "    \n",
        "    X = np.array(xEczema+ xEczemaFair)\n",
        "    y = np.array(yEczema + yEczemaFair)\n",
        "    \n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VL0qD6H8-AY",
        "outputId": "f0d9ad76-fddb-430e-bb68-b3abb9ddd7e2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1UBs7LZ9DEB"
      },
      "source": [
        "def toGray(images):\n",
        "    # rgb2gray converts RGB values to grayscale values by forming a weighted sum of the R, G, and B components:\n",
        "    # 0.2989 * R + 0.5870 * G + 0.1140 * B \n",
        "    # source: https://www.mathworks.com/help/matlab/ref/rgb2gray.html\n",
        "    \n",
        "    images = 0.2989*images[:,:,:,0] + 0.5870*images[:,:,:,1] + 0.1140*images[:,:,:,2]\n",
        "    return images\n",
        "\n",
        "def normalizeImages(images):\n",
        "    # use Histogram equalization to get a better range\n",
        "    # source http://scikit-image.org/docs/dev/api/skimage.exposure.html#skimage.exposure.equalize_hist\n",
        "    images = (images / 255.).astype(np.float32)\n",
        "    \n",
        "    for i in range(images.shape[0]):\n",
        "        images[i] = exposure.equalize_hist(images[i])\n",
        "    \n",
        "    images = images.reshape(images.shape + (1,)) \n",
        "    return images\n",
        "\n",
        "def preprocessData(images):\n",
        "    grayImages = toGray(images)\n",
        "    return normalizeImages(grayImages)\n",
        "\n",
        "def normalizeImages2(images):\n",
        "    for i in range(images.shape[0]):\n",
        "        cv2.normalize(images[i],images[i], alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "    \n",
        "    # if convert to gray scale use this after\n",
        "    print(\"images has shape before\", images.shape)\n",
        "    #images = images.reshape(images.shape + (1,)) \n",
        "    #print(\"images has shape after\", images.shape)\n",
        "    return images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLJlLMWz9JvU"
      },
      "source": [
        "size = 128 #image size\n",
        "classSize = 15000 #number of pictures i want for class YES and class NO .... CHANGE TO 15000 OR 20000\n",
        "\n",
        "scaled_X, y = loadData(size, classSize)\n",
        "scaled_X = preprocessData(scaled_X)\n",
        "\n",
        "n_classes = len(np.unique(y))\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "y = to_categorical(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UJIfhml9J4c"
      },
      "source": [
        "rand_state = np.random.randint(0, 100)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=rand_state)\n",
        "\n",
        "print(\"Number of classes =\", n_classes)\n",
        "print(\"train shape X\", X_train.shape)\n",
        "print(\"train shape y\", y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQvxDy1-9UB0"
      },
      "source": [
        "def karasModel(inputShape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(16, 8, 8, subsample=(4, 4),border_mode='valid', input_shape=inputShape))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(32, 5, 5))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(32, 3, 3))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(240))\n",
        "\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(120))\n",
        "\n",
        "    #model.add(Activation('relu'))\n",
        "    model.add(Dense(2))\n",
        "\n",
        "    model.add(Activation('softmax'))\n",
        "    return model \n",
        "\n",
        "def karasModel2(inputShape):\n",
        "    model = Sequential()\n",
        "    #model.add(Conv2D(16, 8, 8, subsample=(4, 4),border_mode='valid', input_shape=inputShape))\n",
        "    model.add(Conv2D(16, 8, 8, padding='valid', input_shape=inputShape))\n",
        "    model.add(ELU())\n",
        "    model.add(Conv2D(32, 5, 5, padding=\"same\"))\n",
        "    model.add(ELU())\n",
        "    model.add(Conv2D(64, 5, 5, padding=\"same\"))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(.2))\n",
        "    model.add(ELU())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(ELU())\n",
        "    model.add(Dense(2))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e97v9NnE9YrA"
      },
      "source": [
        "from keras.layers.convolutional import Conv2D\n",
        "\n",
        "inputShape = (size, size, 1)\n",
        "model = karasModel2(inputShape)\n",
        "\n",
        "\n",
        "model.compile('adam', 'categorical_crossentropy', ['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=60, validation_split=0.1) #CHANGE EPOCHS TO 60 OR 70\n",
        "\n",
        "\n",
        "metrics = model.evaluate(X_test, y_test)\n",
        "for metric_i in range(len(model.metrics_names)):\n",
        "    metric_name = model.metrics_names[metric_i]\n",
        "    metric_value = metrics[metric_i]\n",
        "    print('{}: {}'.format(metric_name, metric_value))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}